import streamlit as st
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import StrOutputParser
from langgraph.graph import StateGraph, END
from typing import Dict, TypedDict, Annotated, List
import os
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings
from langchain.embeddings import CacheBackedEmbeddings
from langchain.storage import LocalFileStore
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader
from langchain.chains import RetrievalQA
import time
from langchain_community.tools import DuckDuckGoSearchRun
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse
import re
from langchain.prompts import PromptTemplate
from langchain_community.utilities import SerpAPIWrapper
import queue

# ë””ë²„ê·¸ ë¡œê·¸ë¥¼ ì €ì¥í•  ì „ì—­ í
debug_log_queue = queue.Queue()

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# OpenAI API í‚¤ ì„¤ì •
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")

# FAISS ì¸ë±ìŠ¤ ê²½ë¡œ ì„¤ì •
FAISS_INDEX_PATH = "faiss_index"

# ì„ë² ë”© ì„¤ì •
underlying_embeddings = OpenAIEmbeddings()
fs = LocalFileStore("./cache/")
cached_embeddings = CacheBackedEmbeddings.from_bytes_store(
    underlying_embeddings, fs, namespace=underlying_embeddings.model
)

# PDF íŒŒì¼ ë¡œë“œ ë° ë²¡í„° ì €ì¥ì†Œ ìƒì„±
@st.cache_resource
def create_vector_store():
    if os.path.exists(FAISS_INDEX_PATH):
        st.write("ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.")
        vector_store = FAISS.load_local(FAISS_INDEX_PATH, cached_embeddings, allow_dangerous_deserialization=True)
    else:
        st.write("FAISS ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
        vector_store = create_new_vector_store()
    
    return vector_store

def create_new_vector_store():
    st.write("ìƒˆë¡œìš´ ë²¡í„° ì €ì¥ì†Œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
    loader = PyPDFLoader("2023_ìƒë°˜ê¸°_ë²•ë ¹í•´ì„ì‚¬ë¡€ì§‘(ìƒ).pdf")
    documents = loader.load()
    
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    texts = text_splitter.split_documents(documents)
    
    vector_store = FAISS.from_documents(texts, cached_embeddings)
    
    # ë²¡í„° ì €ì¥ì†Œë¥¼ ë¡œì»¬ì— ì €ì¥
    vector_store.save_local(FAISS_INDEX_PATH)
    
    return vector_store

# ë²¡í„° ì €ì¥ì†Œ ìƒì„± ë˜ëŠ” ë¡œë“œ
vector_store = create_vector_store()

# ìƒíƒœ ì •ì˜
class State(TypedDict):
    question: str
    legal_area: str
    search_results: str
    answer: str
    feedback: str
    score: int

# RAG ì²´ì¸ ìƒì„±
llm = ChatOpenAI(model="gpt-4o-mini-2024-07-18", temperature=0) # Don't change the model
rag_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vector_store.as_retriever(),
    return_source_documents=True
)

# ê²€ìƒ‰ ë„êµ¬ ì´ˆê¸°í™”
serpapi_api_key = os.getenv("SERPAPI_API_KEY")  # SerpAPI í‚¤ë¥¼ í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤
search = SerpAPIWrapper(serpapi_api_key=serpapi_api_key)

# ë…¸ë“œ í•¨ìˆ˜ ì •ì˜
def question_analysis(state: State) -> Dict:
    log_debug("ì§ˆë¬¸ ë¶„ì„ ì‹œì‘", 1)
    prompt = ChatPromptTemplate.from_template(
        "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ë²•ë¥  ë¶„ì•¼ë¥¼ íŒë‹¨í•´ì£¼ì„¸ìš”. ì§ˆë¬¸: {question}"
    )
    chain = prompt | llm | StrOutputParser()
    legal_area = chain.invoke({"question": state["question"]})
    log_debug(f"ë¶„ì„ëœ ë²•ë¥  ë¶„ì•¼: {legal_area}", 1)
    return {"legal_area": legal_area}

def search_information(state: State) -> Dict:
    log_debug("ì •ë³´ ê²€ìƒ‰ ì‹œì‘", 2)
    query = f"{state['legal_area']} {state['question']}"
    print("query", query)
    rag_results = rag_chain.invoke(query)
    print("rag_results", rag_results)
    log_debug("ì¸í„°ë„· ê²€ìƒ‰ ì‹œì‘", 2)
    try:
        search_results = search.run(query)
        if not search_results:
            search_results = "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
        log_debug(f"ì¸í„°ë„· ê²€ìƒ‰ ê²°ê³¼: {search_results[:500]}...", 2)
    except Exception as e:
        log_debug(f"ì¸í„°ë„· ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", 2)
        search_results = "ì¸í„°ë„· ê²€ìƒ‰ ì‹¤íŒ¨ ë˜ëŠ” ê²°ê³¼ ì—†ìŒ"
    
    log_debug("RAG ë° ì¸í„°ë„· ê²€ìƒ‰ ì™„ë£Œ", 2)
    
    # ê²€ìƒ‰ ê²°ê³¼ë¥¼ íŒŒì‹±í•˜ì—¬ ì†ŒìŠ¤ ì •ë³´ ì¶”ì¶œ
    sources = parse_search_results(search_results)
    print(sources)
    log_debug(f"state ê°ì²´ ë‚´ìš©: {state}", 2)
    return {
        "rag_results": rag_results['result'] if isinstance(rag_results, dict) and 'result' in rag_results else str(rag_results),
        "search_results": search_results,
        "sources": sources
    }

def combine_information(state: State) -> Dict:
    log_debug("ì •ë³´ ê²°í•© ì‹œì‘", 3)
    combined_info = f"RAG ê²°ê³¼: {state.get('rag_results', 'ê²°ê³¼ ì—†ìŒ')}\n\nê²€ìƒ‰ ê²°ê³¼: {state.get('search_results', 'ê²°ê³¼ ì—†ìŒ')}"
    log_debug(f"combined_info: {combined_info}", 3)
    log_debug(f"state ê°ì²´ ë‚´ìš©: {state}", 3)
    return {
        "combined_info": combined_info,
        "question": state["question"],
        "legal_area": state.get("legal_area", ""),
        "search_results": state.get("search_results", ""),
        "rag_results": state.get("rag_results", "")
    }

def combine_information(state: State) -> Dict:
    log_debug("ì •ë³´ ê²°í•© ì‹œì‘", 3)
    combined_info = f"RAG ê²°ê³¼: {state.get('rag_results', 'ê²°ê³¼ ì—†ìŒ')}\n\nê²€ìƒ‰ ê²°ê³¼: {state.get('search_results', 'ê²°ê³¼ ì—†ìŒ')}"
    log_debug("ì •ë³´ ê²°í•© ì™„ë£Œ", 3)
    return {
        "combined_info": combined_info,
        "question": state["question"],
        "legal_area": state.get("legal_area", ""),
        "search_results": state.get("search_results", ""),
        "rag_results": state.get("rag_results", "")
    }

def parse_search_results(search_results: str) -> List[Dict]:
    sources = []
    urls = re.findall(r'(https?://\S+)', search_results)
    
    for url in urls[:5]:  # ìµœëŒ€ 5ê°œì˜ ì†ŒìŠ¤ë§Œ ì²˜ë¦¬
        try:
            response = requests.get(url, timeout=5)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            title = soup.title.string if soup.title else "ì œëª© ì—†ìŒ"
            description = soup.find('meta', attrs={'name': 'description'})
            description = description['content'] if description else "ì„¤ëª… ì—†ìŒ"
            
            icon = soup.find('link', rel='icon') or soup.find('link', rel='shortcut icon')
            icon_url = icon['href'] if icon else ""
            if icon_url and not icon_url.startswith('http'):
                icon_url = f"{url.split('//', 1)[0]}//{url.split('//', 1)[1].split('/', 1)[0]}{icon_url}"
            
            sources.append({
                "title": title,
                "url": url,
                "description": description[:100] + "..." if len(description) > 100 else description,
                "icon_url": icon_url
            })
        except Exception as e:
            log_debug(f"URL íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", 2)
    
    return sources

# Streamlit UI ë¶€ë¶„
def display_search_results(sources: List[Dict]):
    st.subheader("ê´€ë ¨ ì •ë³´")
    for source in sources:
        with st.expander(source['title']):
            col1, col2 = st.columns([1, 4])
            with col1:
                if source['icon_url']:
                    st.image(source['icon_url'], width=50)
                else:
                    st.write("ğŸŒ")
            with col2:
                st.markdown(f"[{source['url']}]({source['url']})")
                st.write(source['description'])


def generate_answer(state: State) -> Dict:
    log_debug("ë‹µë³€ ìƒì„± ì‹œì‘", 4)
    prompt_template = PromptTemplate(
        input_variables=["question", "combined_info"],
        template="""
        ë‹¹ì‹ ì€ ë²•ë¥  ì „ë¬¸ AI ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ë²•ë¥  ê´€ë ¨ ì§ˆë¬¸ì— ëŒ€í•´ ëª…í™•í•˜ê³  ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.

        ì§ˆë¬¸: {question}

        ê´€ë ¨ ì •ë³´:
        {combined_info}

        ìœ„ì˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ í˜•ì‹ì— ë§ì¶° ë‹µë³€ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”:

        1. ë²•ë¥  ë¶„ì•¼: [ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë²•ë¥  ë¶„ì•¼ë¥¼ ê°„ë‹¨íˆ ëª…ì‹œ]

        2. ë‹µë³€ ìš”ì•½: [ì§ˆë¬¸ì— ëŒ€í•œ í•µì‹¬ ë‹µë³€ì„ 1-2ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½]

        3. ìƒì„¸ ì„¤ëª…:
        [ê´€ë ¨ ë²•ë¥  ì¡°í•­ì´ë‚˜ íŒë¡€ë¥¼ ì¸ìš©í•˜ë©° ë” ìì„¸í•œ ì„¤ëª… ì œê³µ]

        4. ì£¼ì˜ì‚¬í•­:
        [í•´ë‹¹ ì‚¬ì•ˆì— ëŒ€í•œ ì£¼ì˜ì ì´ë‚˜ ì¶”ê°€ë¡œ ê³ ë ¤í•´ì•¼ í•  ì‚¬í•­ ì–¸ê¸‰]

        5. ì¶”ì²œ í–‰ë™:
        [ì§ˆë¬¸ìê°€ ì·¨í•´ì•¼ í•  ë‹¤ìŒ ë‹¨ê³„ë‚˜ í–‰ë™ ì œì•ˆ]

        6. ë©´ì±… ì¡°í•­:
        ì´ ë‹µë³€ì€ ì¼ë°˜ì ì¸ ë²•ë¥  ì •ë³´ ì œê³µ ëª©ì ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìœ¼ë©°, êµ¬ì²´ì ì¸ ë²•ë¥  ìë¬¸ì„ ëŒ€ì²´í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì •í™•í•œ ë²•ë¥  ìë¬¸ì„ ìœ„í•´ì„œëŠ” ë°˜ë“œì‹œ ë³€í˜¸ì‚¬ì™€ ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.
        """
    )
    
    chain = prompt_template | llm | StrOutputParser()
    
    try:
        combined_info = state.get("combined_info", "ì •ë³´ ì—†ìŒ")
        log_debug(f"combined_info: {combined_info}", 4)
        answer = chain.invoke({"question": state["question"], "combined_info": combined_info})
    except Exception as e:
        log_debug(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", 4)
        import traceback
        log_debug(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}", 4)
        answer = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì— ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    log_debug("ë‹µë³€ ìƒì„± ì™„ë£Œ", 4)
    return {"answer": answer}

def evaluate_answer(state: State) -> Dict:
    log_debug("ë‹µë³€ í‰ê°€ ì‹œì‘", 5)
    prompt = ChatPromptTemplate.from_template(
        """
        ë‹¤ìŒ ë‹µë³€ì˜ í’ˆì§ˆì„ 1ë¶€í„° 10ê¹Œì§€ì˜ ì ìˆ˜ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”. 
        ì ìˆ˜ë§Œ ìˆ«ìë¡œ ë°˜í™˜í•´ ì£¼ì„¸ìš”.
        
        ì§ˆë¬¸: {question}
        ë‹µë³€: {answer}
        """
    )
    chain = prompt | llm | StrOutputParser()
    evaluation = chain.invoke({"question": state["question"], "answer": state["answer"]})
    try:
        score = int(evaluation.strip())
        result = "END" if score >= 8 else "generate_answer"
    except ValueError:
        log_debug(f"ì ìˆ˜ ë³€í™˜ ì‹¤íŒ¨. ì›ë³¸ í‰ê°€: {evaluation}", 5)
        score = 0
        result = "generate_answer"
    log_debug(f"í‰ê°€ ê²°ê³¼: {result} (ì ìˆ˜: {score})", 5)
    return {"feedback": str(score), "result": result, "score": score}

# ê·¸ë˜í”„ ì •ì˜
workflow = StateGraph(State)

workflow.add_node("question_analysis", question_analysis)
workflow.add_node("search_information", search_information)
workflow.add_node("combine_information", combine_information)
workflow.add_node("generate_answer", generate_answer)
workflow.add_node("evaluate_answer", evaluate_answer)

workflow.set_entry_point("question_analysis")

# ì—£ì§€ ì •ì˜
workflow.add_edge("question_analysis", "search_information")
workflow.add_edge("search_information", "combine_information")
workflow.add_edge("combine_information", "generate_answer")
workflow.add_edge("generate_answer", "evaluate_answer")

# ì¡°ê±´ë¶€ ì—£ì§€ ì¶”ê°€
workflow.add_conditional_edges(
    "evaluate_answer",
    lambda x: x["result"],
    {
        "generate_answer": "generate_answer",
        "END": END
    }
)

# ê·¸ë˜í”„ ì»´íŒŒì¼
app = workflow.compile()

# ì•± ì¸í„°í˜ì´ìŠ¤
st.title("ë²•ë¥  AI ìƒë‹´ ì‹œìŠ¤í…œ")

# ë„¤ë¹„ê²Œì´ì…˜ ë©”ë‰´ ìƒì„±
page = st.sidebar.selectbox("ëª¨ë“œ ì„ íƒ", ["RAG", "RAG Langgraph"])

# ë””ë²„ê·¸ ëª¨ë“œ í† ê¸€ ì¶”ê°€
debug_mode = st.sidebar.checkbox("ë””ë²„ê·¸ ëª¨ë“œ", value=True)

# ë””ë²„ê·¸ ë¡œê·¸ë¥¼ ìœ„í•œ ì»¨í…Œì´ë„ˆ ìƒì„±
debug_container = st.sidebar.container()

# ì‹œê°„ ì¸¡ì •ì„ ìœ„í•œ ì»¨í…Œì´ë„ˆ ìƒì„±
timer_container = st.sidebar.empty()

# ë¡œê·¸ ì¶œë ¥ í•¨ìˆ˜
def log_debug(message, step=None):
    if step:
        message = f"Step {step}: {message}"
    debug_log_queue.put(message)
    print(message)  # í„°ë¯¸ë„ì—ë„ ì¶œë ¥

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []
if "debug_logs" not in st.session_state:
    st.session_state.debug_logs = []

# ì´ì „ ë©”ì‹œì§€ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥
if prompt := st.chat_input("ë²•ë¥  ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”:"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # AI ì‘ë‹µ ìƒì„±
    with st.chat_message("assistant"):
        full_response = ""
        message_placeholder = st.empty()

        if page == "RAG":
            # RAG ëª¨ë“œ: FAISS ì¸ë±ìŠ¤ í…ŒìŠ¤íŠ¸
            try:
                log_debug("RAG ëª¨ë“œ ì‹œì‘", 0)
                start_time = time.time()
                
                log_debug("RAG ì²´ì¸ ì‹¤í–‰", 1)
                response = rag_chain.invoke(prompt)
                log_debug("ì¸í„°ë„· ê²€ìƒ‰ ì‹¤í–‰", 2)
                search_results = search.run(prompt)
                
                full_response = ""
                message_placeholder = st.empty()
                full_response = ""
                
                log_debug("ë‹µë³€ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘", 3)
                for chunk in response['result'].split():
                    full_response += chunk + " "
                    if chunk.endswith(('.', '!', '?', '\n')):
                        full_response += "\n\n"
                    time.sleep(0.05)
                    message_placeholder.markdown(full_response + "â–Œ")
                
                message_placeholder.markdown(full_response)
                
                log_debug("ì†ŒìŠ¤ ì •ë³´ ì¶”ì¶œ ë° í‘œì‹œ", 4)
                sources = []
                urls = extract_urls(search_results)
                for url in urls[:5]:
                    title, image_url = get_webpage_info(url)
                    sources.append({
                        "type": "ì¸í„°ë„· ê²€ìƒ‰",
                        "url": url,
                        "title": title,
                        "image_url": image_url
                    })
                
                display_sources(sources)
                
                elapsed_time = time.time() - start_time
                timer_container.markdown(f"â±ï¸ ê²½ê³¼ ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
                
                log_debug("RAG ëª¨ë“œ ì¢…ë£Œ", 6)
            except Exception as e:
                st.error(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                log_debug(f"ì˜¤ë¥˜ ìƒì„¸ ì •ë³´: {type(e).__name__}, {str(e)}")
                import traceback
                log_debug(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
        
        elif page == "RAG Langgraph":
            # RAG Langgraph ëª¨ë“œ
            initial_state = State(
                question=prompt,
                legal_area="",
                rag_results="",
                search_results="",
                combined_info="",
                answer="",
                feedback="",
                score=0  # ì´ˆê¸° ì ìˆ˜ ì¶”ê°€
            )
             
            try:
                log_debug("RAG Langgraph ëª¨ë“œ ì‹œì‘", 0)
                start_time = time.time()
                max_iterations = 10  # ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ ì„¤ì •
                full_response = ""
                message_placeholder = st.empty()
                
                for step, output in enumerate(app.stream(initial_state), 1):
                    if step > max_iterations:
                        log_debug(f"ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ ({max_iterations})ì— ë„ë‹¬í•˜ì—¬ ì¢…ë£Œ", step)
                        break
                    
                    log_debug(f"Step {step} ì‹¤í–‰", step)
                    if isinstance(output, Dict):
                        for key, value in output.items():
                            if key in ["legal_area", "rag_results", "search_results", "combined_info", "answer", "feedback", "score"]:
                                if key == "score":
                                    full_response += f"í˜„ì¬ ë‹µë³€ ì ìˆ˜: {value}\n\n"
                                elif key == "answer":
                                    # ë‹µë³€ì„ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ìŠ¤íŠ¸ë¦¬ë°
                                    current_answer = ""
                                    for word in value.split():
                                        current_answer += word + " "
                                        full_response = f"{full_response}{current_answer}â–Œ"
                                        message_placeholder.markdown(full_response)
                                        time.sleep(0.05)
                                    full_response = full_response[:-1] + "\n\n"  # ë§ˆì§€ë§‰ 'â–Œ' ì œê±° ë° ì¤„ë°”ê¿ˆ ì¶”ê°€
                                else:
                                    full_response += f"{key.capitalize()}:\n{value}\n\n"
                                
                                message_placeholder.markdown(full_response)
                                
                                log_message = f"{key}: {value[:500]}..." if isinstance(value, str) and len(value) > 500 else f"{key}: {value}"
                                log_debug(log_message, step)
                                st.session_state.debug_logs.append(log_message)
                            
                        if key == "sources":
                            log_debug("ì†ŒìŠ¤ ì •ë³´ í‘œì‹œ", step)
                            display_search_results(value)
                    
                    elapsed_time = time.time() - start_time
                    timer_container.markdown(f"â±ï¸ ê²½ê³¼ ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
                       
                    if "result" in output and output["result"] == "END":
                        log_debug(f"RAG Langgraph ëª¨ë“œ ì¢…ë£Œ (ìµœì¢… ì ìˆ˜: {output.get('score', 'N/A')})", step + 1)
                        # ìµœì¢… ë‹µë³€ ì¶œë ¥
                        st.markdown("### ìµœì¢… ë‹µë³€")
                        st.markdown(output.get("answer", "ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."))
                        break
                    
            except Exception as e:
                st.error(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                log_debug(f"ì˜¤ë¥˜ ìƒì„¸ ì •ë³´: {type(e).__name__}, {str(e)}")
                import traceback
                log_debug(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
            
            # ìµœì¢… ë‹µë³€ì´ ì¶œë ¥ë˜ì§€ ì•Šì•˜ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ì¶”ê°€ ì¶œë ¥
            if "answer" not in locals():
                st.warning("ë‹µë³€ ìƒì„± ê³¼ì •ì—ì„œ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")
            elif not st.session_state.messages[-1]["content"]:
                st.markdown("### ìµœì¢… ë‹µë³€")
                st.markdown(locals().get("answer", "ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."))
            
            full_response = ""
            message_placeholder = st.empty()

            # ë§ˆì§€ë§‰ ë©”ì‹œì§€ì˜ ë‚´ìš©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤
            if st.session_state.messages and isinstance(st.session_state.messages[-1], dict):
                last_message = st.session_state.messages[-1].get('content', '')
                for chunk in last_message.split():
                    full_response += chunk + " "
                    if chunk.endswith(('.', '!', '?', '\n')):
                        full_response += "\n\n"
                    time.sleep(0.05)
                    message_placeholder.markdown(full_response + "â–Œ")

                message_placeholder.markdown(full_response)

                # ë§ˆì§€ë§‰ ë©”ì‹œì§€ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤
                st.session_state.messages[-1]['content'] = full_response
            else:
                st.warning("ë©”ì‹œì§€ê°€ ì—†ê±°ë‚˜ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")

        # ë””ë²„ê·¸ ë¡œê·¸ í‘œì‹œ
        if debug_mode:
            debug_logs = []
            while not debug_log_queue.empty():
                debug_logs.append(debug_log_queue.get())
            debug_container.text_area("ë””ë²„ê·¸ ë¡œê·¸", "\n".join(debug_logs), height=300)

# í”¼ë“œë°± ìˆ˜ì§‘ (ì‚¬ì´ë“œë°”ë¡œ ì´ë™)
with st.sidebar:
    st.subheader("í”¼ë“œë°±")
    feedback = st.text_area("ë‹µë³€ì— ëŒ€í•œ í”¼ë“œë°±ì„ ë‚¨ê²¨ì£¼ì„¸ìš”:")
    if st.button("í”¼ë“œë°± ì œì¶œ"):
        if feedback:
            st.success("í”¼ë“œë°±ì´ ì œì¶œë˜ì—ˆìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!")
            log_debug(f"í”¼ë“œë°± ì œì¶œ: {feedback}")
        else:
            st.warning("í”¼ë“œë°±ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")



